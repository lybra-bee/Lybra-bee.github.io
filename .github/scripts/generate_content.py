import requests
import os
import time
import argparse
import re
import html
from datetime import datetime
import feedparser
from bs4 import BeautifulSoup

def clean_text(text):
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç HTML —Ç–µ–≥–æ–≤ –∏ –º—É—Å–æ—Ä–∞"""
    if not text:
        return ""
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML entities
    text = html.unescape(text)
    
    # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
    text = re.sub(r'<[^>]*>', '', text)
    
    # –£–¥–∞–ª—è–µ–º URL –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'[^\w\s–∞-—è–ê-–Ø—ë–Å.,!?;-]', ' ', text)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def get_trending_topics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ç–µ–º"""
    try:
        topics = []
        
        # –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã –ø–æ AI –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º
        current_trends = [
            "–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ GPT-5 –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞",
            "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ò–ò –≤ –±–∏–∑–Ω–µ—Å–µ 2024",
            "–ù–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ",
            "–ò–ò –∏ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: –Ω–æ–≤—ã–µ –≤—ã–∑–æ–≤—ã",
            "–≠—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞",
            "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
            "–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
            "–ò–ò –≤ –º–µ–¥–∏—Ü–∏–Ω–µ –∏ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏",
            "–ë—É–¥—É—â–µ–µ —Ä–∞–±–æ—Ç—ã –≤ —ç–ø–æ—Ö—É –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞",
            "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π",
            "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã 2024",
            "–ö–≤–∞–Ω—Ç–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
            "–ò–ò –¥–ª—è –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏ —ç–∫–æ–ª–æ–≥–∏–∏",
            "–ù–µ–π—Ä–æ–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –º–æ–∑–≥–æ–º",
            "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏",
            "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –∏ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞",
            "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
            "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ AI —Å–∏—Å—Ç–µ–º—ã",
            "Edge computing –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"
        ]
        
        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–º—ã –∏–∑ RSS (—Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π)
        rss_feeds = [
            "https://habr.com/ru/rss/articles/?fl=ru",
            "https://news.ycombinator.com/rss"
        ]
        
        for feed_url in rss_feeds:
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries[:3]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
                    if 'title' in entry:
                        clean_title = clean_text(entry.title)
                        if clean_title and len(clean_title) > 20 and len(clean_title) < 120:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ HTML –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Å—ã–ª–æ–∫
                            if not any(char in clean_title for char in ['<', '>', 'http', '://']):
                                topics.append(clean_title)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS: {e}")
                continue
        
        topics.extend(current_trends)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_topics = []
        seen = set()
        for topic in topics:
            if (topic and topic not in seen and 
                15 <= len(topic) <= 100 and
                not any(char in topic for char in ['<', '>', 'http', '://'])):
                unique_topics.append(topic)
                seen.add(topic)
        
        return unique_topics[:20]
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Ö–æ—Ä–æ—à–∏–µ —Ç–µ–º—ã
        return [
            "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –≤ –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ 2024",
            "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ò–ò –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ",
            "–ù–µ–π—Ä–æ—Å–µ—Ç–∏ –≤ –±–∏–∑–Ω–µ—Å-–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏",
            "–ë—É–¥—É—â–µ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞"
        ]

def generate_article(topic, api_type="groq"):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ Groq –∏–ª–∏ OpenRouter"""
    try:
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–º—ã
        if (not topic or len(topic) < 10 or len(topic) > 150 or
            any(char in topic for char in ['<', '>', 'http', '://'])):
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é —Ç–µ–º—É: {topic}")
            return None
            
        if api_type == "groq":
            api_key = os.getenv('GROQ_API_KEY')
            url = "https://api.groq.com/openai/v1/chat/completions"
            model = "llama-3.1-70b-versatile"
        else:
            api_key = os.getenv('OPENROUTER_API_KEY')
            url = "https://openrouter.ai/api/v1/chat/completions"
            model = "meta-llama/llama-3.1-70b"
        
        if not api_key:
            print(f"‚ùå {api_type.upper()}_API_KEY not found")
            return None
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        if api_type == "openrouter":
            headers.update({
                "HTTP-Referer": "https://github.com/lybra-bee/lybra-bee.github.io",
                "X-Title": "AI Content Generator"
            })
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        system_prompt = """–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ø–∏—Å–∞—Ç–µ–ª—å –∏ —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É. 
–ü–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω—ã–µ, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ò—Å–ø–æ–ª—å–∑—É–π Markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ."""

        payload = {
            "model": model,
            "messages": [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user", 
                    "content": f"–ù–∞–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç—å—é –Ω–∞ —Ç–µ–º—É: '{topic}'. –û–±—ä–µ–º: 1000-1500 —Å–ª–æ–≤. –§–æ—Ä–º–∞—Ç: Markdown."
                }
            ],
            "max_tokens": 3000,
            "temperature": 0.7
        }
        
        print(f"üìù –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Å—Ç–∞—Ç—å—é —á–µ—Ä–µ–∑ {api_type.upper()}: {topic}")
        response = requests.post(url, headers=headers, json=payload, timeout=90)
        response.raise_for_status()
        
        content = response.json()['choices'][0]['message']['content']
        print(f"‚úÖ –°—Ç–∞—Ç—å—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞! –î–ª–∏–Ω–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return content
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏ ({api_type}): {e}")
        return None

def generate_image(prompt):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ Hugging Face"""
    try:
        HF_TOKEN = os.getenv('HF_API_TOKEN')
        if not HF_TOKEN:
            print("‚ö†Ô∏è HF_API_TOKEN not found, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return None
        
        # –£–ø—Ä–æ—â–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_prompt = f"{prompt}, digital art, professional, 4k quality"
        print(f"üé® –ì–µ–Ω–µ—Ä–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_prompt}")
        
        API_URL = "https://api-inference.huggingface.co/models/runwayml/stable-diffusion-v1-5"
        headers = {"Authorization": f"Bearer {HF_TOKEN}"}
        
        payload = {
            "inputs": image_prompt,
            "parameters": {
                "width": 512,
                "height": 512
            }
        }
        
        response = requests.post(API_URL, headers=headers, json=payload, timeout=60)
        
        if response.status_code == 200:
            filename = f"image_{int(time.time())}.png"
            filepath = f"static/images/posts/{filename}"
            
            with open(filepath, "wb") as f:
                f.write(response.content)
            
            print(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ: {filepath}")
            return filepath
        else:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {response.status_code}")
            return None
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        return None

def save_article(content, topic, image_path=None):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ —Å front matter"""
    try:
        import re
        from datetime import date
        
        # –°–æ–∑–¥–∞–µ–º SEO-–¥—Ä—É–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        clean_topic = re.sub(r'[^a-zA-Z0-9–∞-—è–ê-–Ø\s]', '', topic)
        filename = re.sub(r'\s+', '-', clean_topic.lower())[:50]
        
        today = date.today().strftime("%Y-%m-%d")
        full_filename = f"content/posts/{today}-{filename}.md"
        
        # Front matter –¥–ª—è Hugo
        front_matter = f"""---
title: "{topic}"
date: {today}
draft: false
description: "–°—Ç–∞—Ç—å—è –æ {topic}"
categories: ["–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç"]
tags: ["ai", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"]
"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if image_path:
            image_filename = os.path.basename(image_path)
            front_matter += f"image: \"/images/posts/{image_filename}\"\n"
        
        front_matter += "---\n"
        
        full_content = front_matter + content
        
        with open(full_filename, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        print(f"üíæ –°—Ç–∞—Ç—å—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {full_filename}")
        return full_filename
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")
        return None

def main():
    parser = argparse.ArgumentParser(description='AI Content Generator')
    parser.add_argument('--count', type=int, default=1, help='Number of articles')
    parser.add_argument('--debug', action='store_true', help='Debug mode')
    
    args = parser.parse_args()
    
    print("üîÑ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
    print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π: {args.count}")
    
    # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã
    print("üì° –ü–æ–ª—É—á–∞—é –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã...")
    topics = get_trending_topics()
    
    if not topics:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–º—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
        topics = [
            "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –≤ –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ",
            "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ò–ò –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ"
        ]
    
    selected_topics = topics[:args.count]
    print(f"üéØ –í—ã–±—Ä–∞–Ω–Ω—ã–µ —Ç–µ–º—ã: {selected_topics}")
    
    generated_count = 0
    
    for i, topic in enumerate(selected_topics):
        print(f"\nüìñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {i+1}/{len(selected_topics)}: {topic}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å—é
        content = generate_article(topic, "groq")
        if not content:
            content = generate_article(topic, "openrouter")
        
        if content:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_path = generate_image(topic)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—å—é
            article_path = save_article(content, topic, image_path)
            if article_path:
                generated_count += 1
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: —Å—Ç–∞—Ç—å—è + {'–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ' if image_path else '–±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è'}")
        else:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç—å—é")
    
    print(f"\nüéâ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –£—Å–ø–µ—à–Ω–æ: {generated_count}/{len(selected_topics)}")

if __name__ == "__main__":
    main()
